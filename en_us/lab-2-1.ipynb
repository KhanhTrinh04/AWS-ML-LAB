{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1: Applying ML to an NLP Problem\n",
    "\n",
    "In this lab, you will use the built-in machine learning (ML) model in Amazon SageMaker, __LinearLearner__, to predict the __isPositive__ field of the review dataset.\n",
    "\n",
    "## Introducing the business scenario\n",
    "You work for an online retail store that wants to improve customer engagement for customers who have posted negative reviews. The company wants to detect negative reviews and assign these reviews to a customer service agent to address.\n",
    "\n",
    "You are tasked with solving part of this problem by using ML to detect negative reviews. You were given access to a dataset that contains reviews, which have been classified as positive or negative. You will use this dataset to train an ML model to predict the sentiment of new reviews.\n",
    "\n",
    "## About this dataset\n",
    "The [AMAZON-REVIEW-DATA-CLASSIFICATION.csv](https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/examples) file contains actual reviews of products, and these reviews include both text data and numeric data. Each review is labeled as _positive (1)_ or _negative (0)_.\n",
    "\n",
    "The dataset contains the following features:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "The dataset for this lab is being provided to you by permission of Amazon and is subject to the terms of the Amazon License and Access (available at https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000). You are expressly prohibited from copying, modifying, selling, exporting, or using this dataset in any way other than for the purpose of completing this course.\n",
    "\n",
    "## Lab steps\n",
    "\n",
    "To complete this lab, you will follow these steps:\n",
    "\n",
    "1. [Reading the dataset](#1.-Reading-the-dataset)\n",
    "2. [Performing exploratory data analysis](#2.-Performing-exploratory-data-analysis)\n",
    "3. [Text processing: Removing stopwords and stemming](#3.-Text-processing:-Removing-stopwords-and-stemming)\n",
    "4. [Splitting training, validation, and test data](#4.-Splitting-training,-validation,-and-test-data)\n",
    "5. [Processing data with pipelines and a ColumnTransformer](#5.-Processing-data-with-pipelines-and-a-ColumnTransformer)\n",
    "6. [Training a classifier with a built-in SageMaker algorithm](#6.-Training-a-classifier-with-a-built-in-SageMaker-algorithm)\n",
    "7. [Evaluating the model](#7.-Evaluating-the-model)\n",
    "8. [Deploying the model to an endpoint](#8.-Deploying-the-model-to-an-endpoint)\n",
    "9. [Testing the endpoint](#9.-Testing-the-endpoint)\n",
    "10. [Cleaning up model artifacts](#10.-Cleaning-up-model-artifacts)\n",
    "    \n",
    "## Submitting your work\n",
    "\n",
    "1. In the lab console, choose **Submit** to record your progress and when prompted, choose **Yes**.\n",
    "\n",
    "1. If the results don't display after a couple of minutes, return to the top of these instructions and choose **Grades**.\n",
    "\n",
    "     **Tip**: You can submit your work multiple times. After you change your work, choose **Submit** again. Your last submission is what will be recorded for this lab.\n",
    "\n",
    "1. To find detailed feedback on your work, choose **Details** followed by **View Submission Report**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by installing/upgrading pip, sagemaker, and scikit-learn.\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) is an open source machine learning library. It provides various tools for model fitting, data preprocessing, model selection and evaluation and many other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (25.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.251.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.251.1-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: attrs<26,>=24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (25.3.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.39.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.40.21)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.1.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.116.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: graphene<4,>=3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.4.3)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<3,>=2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: packaging<25,>=23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (24.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.3.2)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.4)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.4.0)\n",
      "Requirement already satisfied: protobuf<6.32,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.31.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.1.1)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.32.5)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.57)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.1.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.5.0)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.35.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.21 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.39.5->sagemaker) (1.40.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.39.5->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.39.5->sagemaker) (0.13.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.21->boto3<2.0,>=1.39.5->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.6)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from graphene<4,>=3->sagemaker) (4.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.23.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from omegaconf<3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.21->boto3<2.0,>=1.39.5->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.9.2)\n",
      "Requirement already satisfied: rich<15.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (14.1.0)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.27.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.23.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<15.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2025.8.3)\n",
      "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fastapi->sagemaker) (0.47.3)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from starlette<0.48.0,>=0.40.0->fastapi->sagemaker) (4.10.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2025.2)\n",
      "Requirement already satisfied: ppft>=1.7.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.7)\n",
      "Requirement already satisfied: dill>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: pox>=0.3.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.18)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn->sagemaker) (0.16.0)\n",
      "Downloading sagemaker-2.251.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.251.0\n",
      "    Uninstalling sagemaker-2.251.0:\n",
      "      Successfully uninstalled sagemaker-2.251.0\n",
      "Successfully installed sagemaker-2.251.1\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.40.21)\n",
      "Collecting botocore\n",
      "  Downloading botocore-1.40.24-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.17.0)\n",
      "Downloading botocore-1.40.24-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m182.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.40.21\n",
      "    Uninstalling botocore-1.40.21:\n",
      "      Successfully uninstalled botocore-1.40.21\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.42.21 requires botocore==1.40.21, but you have botocore 1.40.24 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed botocore-1.40.24\n",
      "Requirement already satisfied: awscli in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (1.42.21)\n",
      "Collecting awscli\n",
      "  Downloading awscli-1.42.24-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: botocore==1.40.24 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (1.40.24)\n",
      "Requirement already satisfied: docutils<=0.19,>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.19)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.13.1)\n",
      "Requirement already satisfied: PyYAML<6.1,>=3.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (6.0.2)\n",
      "Requirement already satisfied: colorama<0.4.7,>=0.2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (0.4.6)\n",
      "Requirement already satisfied: rsa<4.8,>=3.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from awscli) (4.7.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.40.24->awscli) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.40.24->awscli) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore==1.40.24->awscli) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.40.24->awscli) (1.17.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.1)\n",
      "Downloading awscli-1.42.24-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: awscli\n",
      "  Attempting uninstall: awscli\n",
      "    Found existing installation: awscli 1.42.21\n",
      "    Uninstalling awscli-1.42.21:\n",
      "      Successfully uninstalled awscli-1.42.21\n",
      "Successfully installed awscli-1.42.24\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "#Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade botocore\n",
    "!pip install --upgrade awscli\n",
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You will use the __pandas__ library to read the dataset. [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html) is a popular python library that is used for data analysis. It provides data manipulation, cleaning, and data wrangling features as well as visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (70000, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the first five rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURCHASED FOR YOUNGSTER WHO\\r\\nINHERITED MY \"T...</td>\n",
       "      <td>IDEAL FOR BEGINNER!</td>\n",
       "      <td>True</td>\n",
       "      <td>1361836800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unable to open or use</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1452643200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
       "      <td>Dont buy it!</td>\n",
       "      <td>True</td>\n",
       "      <td>1433289600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>I attempted to install this OS on two differen...</td>\n",
       "      <td>True</td>\n",
       "      <td>1518912000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've spent 14 fruitless hours over the past tw...</td>\n",
       "      <td>Do NOT Download.</td>\n",
       "      <td>True</td>\n",
       "      <td>1441929600</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  PURCHASED FOR YOUNGSTER WHO\\r\\nINHERITED MY \"T...   \n",
       "1                              unable to open or use   \n",
       "2   Waste of money!!! It wouldn't load to my system.   \n",
       "3  I attempted to install this OS on two differen...   \n",
       "4  I've spent 14 fruitless hours over the past tw...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                                IDEAL FOR BEGINNER!      True  1361836800   \n",
       "1                                          Two Stars      True  1452643200   \n",
       "2                                       Dont buy it!      True  1433289600   \n",
       "3  I attempted to install this OS on two differen...      True  1518912000   \n",
       "4                                   Do NOT Download.      True  1441929600   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         0.0  \n",
       "2   0.000000         0.0  \n",
       "3   0.000000         0.0  \n",
       "4   1.098612         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the options in the notebook to display more of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PURCHASED FOR YOUNGSTER WHO\\r\\nINHERITED MY \"TOO sMALL FOR ME\"\\r\\nLAPTOP.  IDEAL FOR LEARNING A\\r\\nFUTURE GOOD SKILL.  HER CHOICE\\r\\nOF BOOKS IS A PLUS AS WAS THIS BOOK!</td>\n",
       "      <td>IDEAL FOR BEGINNER!</td>\n",
       "      <td>True</td>\n",
       "      <td>1361836800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unable to open or use</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1452643200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Waste of money!!! It wouldn't load to my system.</td>\n",
       "      <td>Dont buy it!</td>\n",
       "      <td>True</td>\n",
       "      <td>1433289600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I attempted to install this OS on two different PCs. it will not complete the install.\\r\\nWhen it gets to the page to select the language, and country the mouse and keyboard become non-functional.</td>\n",
       "      <td>I attempted to install this OS on two different PCs ...</td>\n",
       "      <td>True</td>\n",
       "      <td>1518912000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've spent 14 fruitless hours over the past two days fruitlessly attempting to install this software on my computer and nothing I've found has worked. I need the software to type proficiently due to disability, and it will not install. The download itself seems to be a corrupted file, I have a fair amount of computer skills and no amount of tinkering has made the program work, so, judging by other reviews, it must be the program itself.</td>\n",
       "      <td>Do NOT Download.</td>\n",
       "      <td>True</td>\n",
       "      <td>1441929600</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                 reviewText  \\\n",
       "0                                                                                                                                                                                                                                                                                 PURCHASED FOR YOUNGSTER WHO\\r\\nINHERITED MY \"TOO sMALL FOR ME\"\\r\\nLAPTOP.  IDEAL FOR LEARNING A\\r\\nFUTURE GOOD SKILL.  HER CHOICE\\r\\nOF BOOKS IS A PLUS AS WAS THIS BOOK!   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                     unable to open or use   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                          Waste of money!!! It wouldn't load to my system.   \n",
       "3                                                                                                                                                                                                                                                      I attempted to install this OS on two different PCs. it will not complete the install.\\r\\nWhen it gets to the page to select the language, and country the mouse and keyboard become non-functional.   \n",
       "4  I've spent 14 fruitless hours over the past two days fruitlessly attempting to install this software on my computer and nothing I've found has worked. I need the software to type proficiently due to disability, and it will not install. The download itself seems to be a corrupted file, I have a fair amount of computer skills and no amount of tinkering has made the program work, so, judging by other reviews, it must be the program itself.   \n",
       "\n",
       "                                                   summary  verified  \\\n",
       "0                                      IDEAL FOR BEGINNER!      True   \n",
       "1                                                Two Stars      True   \n",
       "2                                             Dont buy it!      True   \n",
       "3  I attempted to install this OS on two different PCs ...      True   \n",
       "4                                         Do NOT Download.      True   \n",
       "\n",
       "         time  log_votes  isPositive  \n",
       "0  1361836800   0.000000         1.0  \n",
       "1  1452643200   0.000000         0.0  \n",
       "2  1433289600   0.000000         0.0  \n",
       "3  1518912000   0.000000         0.0  \n",
       "4  1441929600   1.098612         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at specific entries if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             reviewText  \\\n",
      "580  i had no intention of using this product; like others, i just got it as part of a bundle that gave me a substantial rebate. since so many other people have written about their problems actually getting the rebate, i figured i should chime in with my experience. if you follow the direction precisely you will get your rebate. just don't ever actually install the software. and put a reminder on your calendar to go and cancel your automatic subscription renewal. their rebate tracking is actually pretty good. it took about two weeks from when i sent it (snail mail) to when their system said they received it. then it took another two weeks for them to approve it. when i filed the rebate i opted for the expedited version which cost me 10% of the rebate (deducted directly from the rebate-no money up front). after approval, they mailed it within four days and a received it a couple of days later. It comes in a completely nondescript envelope so it's easy to miss. But it's a real, live AmEX prepaid gift card. I used it last night in fact. And today I remembered to go cancel my automatic renewal service. Contrary to what some others have said, it took on the first time and wasn't a big deal. I used a throw away email address when I signed up (though I haven't received any email from them since) so it took me longer to figure out what my username was to sign in than it did for me to actually cancel.   \n",
      "\n",
      "                                                                                         summary  \\\n",
      "580  worth the rebate if you can follow instructions. i wouldn't install the software though....   \n",
      "\n",
      "     verified        time  log_votes  isPositive  \n",
      "580     False  1404172800        0.0         1.0  \n"
     ]
    }
   ],
   "source": [
    "print(df.loc[[580]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to know what data types you are dealing with. You can use `dtypes` on the dataframe to display the types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText     object\n",
       "summary        object\n",
       "verified         bool\n",
       "time            int64\n",
       "log_votes     float64\n",
       "isPositive    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performing exploratory data analysis\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You will now look at the target distribution for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isPositive\n",
       "1.0    43692\n",
       "0.0    26308\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business problem is concerned with finding the negative reviews (_0_). However, the model tuning for linear learner defaults to finding positive values (_1_). You can make this process run more smoothly by switching the negative values (_0_) and positive values (_1_). By doing so, you can tune the model more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isPositive\n",
       "0.0    43692\n",
       "1.0    26308\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.replace({0:1, 1:0})\n",
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the number of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewText    12\n",
       "summary       15\n",
       "verified       0\n",
       "time           0\n",
       "log_votes      0\n",
       "isPositive     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text fields have missing values. Typically, you would decide what to do with these missing values. You could remove the data or fill it with some standard text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text processing: Removing stopwords and stemming\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "In this task, you will remove some of the stopwords, and perform stemming on the text data. You are normalizing the data to reduce the amount of different information you have to deal with.\n",
    "\n",
    "[nltk](https://www.nltk.org/) is a popular platform for working with human language data. It provides interfaces and functions for processing text for classification, tokenization, stemming, tagging, parsin, and semantic reasoning. \n",
    "\n",
    "Once imported, you can download only the functionality you need. In this example, you will use:\n",
    "\n",
    "- **punkt** is a sentence tokenizer\n",
    "- **stopwords** provides a list of stopwords you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will create the processes for removing stopwords and cleaning the text in the following section. The Natural Language Toolkit (NLTK) library provides a list of common stopwords. You will use the list, but you will first remove some of the words from that list. The stopwords that you keep in the text are useful for determining sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get a list of stopwords from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for your problem. You don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n",
    "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n",
    "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n",
    "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n",
    "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n",
    "\n",
    "# New stopword list\n",
    "stopwords = [word for word in stop if word not in excluding]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snowball stemmer will stem words. For example, 'walking' will be stemmed to 'walk'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must perform a few other normalization tasks on the data. The following function will:\n",
    "\n",
    "- Replace any missing values with an empty string\n",
    "- Convert the text to lowercase\n",
    "- Remove any leading or training whitespace\n",
    "- Remove any extra space and tabs\n",
    "- Remove any HTML markup\n",
    "\n",
    "In the `for` loop, any words that are __NOT__ numeric, longer than 2 characters, and not part of the list of stop words will be kept and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = ''\n",
    "            \n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # Applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stopwords\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "        \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting training, validation, and test data\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "In this step, you will split the dataset into training (80 percent), validation (10 percent), and test (10 percent) by using the sklearn [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function.\n",
    "\n",
    "The training data will be used to train the model which is then tested with the test data. The validation set is used once the model has been trained to give you metrics on how the model might perform on real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[['reviewText', 'summary', 'time', 'log_votes']],\n",
    "                                                  df['isPositive'],\n",
    "                                                  test_size=0.20,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=324\n",
    "                                                 )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,\n",
    "                                                y_val,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the dataset split, you can now run the `process_text` function defined above on each of the text features in the training, test, and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the reviewText fields\n",
      "Processing the summary fields\n"
     ]
    }
   ],
   "source": [
    "print('Processing the reviewText fields')\n",
    "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n",
    "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n",
    "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n",
    "\n",
    "print('Processing the summary fields')\n",
    "X_train['summary'] = process_text(X_train['summary'].tolist())\n",
    "X_val['summary'] = process_text(X_val['summary'].tolist())\n",
    "X_test['summary'] = process_text(X_test['summary'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing data with pipelines and a ColumnTransformer\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You will often perform many tasks on data before you use it to train a model. These steps must also be done on any data that's used for inference after the model is deployed. A good way of organizing these steps is to define a _pipeline_. A pipeline is a collection of processing tasks that will be performed on the data. Different pipelines can be created to process different fields. Because you are working with both text and numeric data, you can define the following pipelines:\n",
    "\n",
    "   * For the numerical features pipeline, the __numerical_processor__ uses a MinMaxScaler. (You don't need to scale features when you use decision trees, but it's a good idea to see how to use more data transforms.) If you want to perform different types of processing on different numerical features, you should build different pipelines, like the ones that are shown for the two text features.\n",
    "   * For the text features pipeline, the __text_processor__ uses `CountVectorizer()` for the text fields.\n",
    "   \n",
    "The selective preparations of the dataset features are then put together into a collective ColumnTransformer, which will be used with in a pipeline along with an estimator. This process ensures that the transforms are performed automatically on the raw data when you fit the model or make predictions. (For example, when you evaluate the model on a validation dataset via cross-validation, or when you make predictions on a test dataset in the future.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = ['time',\n",
    "                      'log_votes']\n",
    "\n",
    "text_features = ['summary',\n",
    "                 'reviewText']\n",
    "\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'isPositive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets shapes before processing:  (56000, 4) (7000, 4) (7000, 4)\n",
      "Datasets shapes after processing:  (56000, 202) (7000, 202) (7000, 202)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) \n",
    "                                ])\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the number of features in the datasets went from 4 to 202."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7561223 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        1.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 1.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a classifier with a built-in SageMaker algorithm\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "In this step, you will call the Sagemaker `LinearLearner()` algorithm with the following options:\n",
    "* __Permissions -__ `role` is set to the AWS Identity and Access Management (IAM) role from the current environment.\n",
    "* __Compute power -__ You will use the `train_instance_count` parameter and the `train_instance_type` parameter. This example uses an `ml.m4.xlarge` resource for training. You can change the instance type depending on your needs. (For example, you could use GPUs for neural networks.) \n",
    "* __Model type -__ `predictor_type` is set to __`binary_classifier`__ because you are working with a binary classification problem. You could use __`multiclass_classifier`__ if three or more classes are involved, or you could use __`regressor`__ for a regression problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "linear_classifier = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                           instance_count=1,\n",
    "                                           instance_type='ml.m4.xlarge',\n",
    "                                           predictor_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the training, validation, and test parts of the estimator, you can use the `record_set()` function of the `binary_estimator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = linear_classifier.record_set(X_train.astype('float32'),\n",
    "                                            y_train.values.astype('float32'),\n",
    "                                            channel='train')\n",
    "val_records = linear_classifier.record_set(X_val.astype('float32'),\n",
    "                                          y_val.values.astype('float32'),\n",
    "                                          channel='validation')\n",
    "test_records = linear_classifier.record_set(X_test.astype('float32'),\n",
    "                                           y_test.values.astype('float32'),\n",
    "                                           channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit()` function applies a distributed version of the Stochastic Gradient Descent (SGD) algorithm, and you are sending the data to it. The logs were disabled with `logs=False`. You can remove that parameter to see more details about the process. __This process takes about 3-4 minutes on an ml.m4.xlarge instance.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Same images used for training and inference. Defaulting to image scope: inference.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: linear-learner-2025-09-05-10-35-42-784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-05 10:35:43 Starting - Starting the training job...\n",
      "2025-09-05 10:36:06 Starting - Preparing the instances for training...\n",
      "2025-09-05 10:36:32 Downloading - Downloading input data...\n",
      "2025-09-05 10:37:02 Downloading - Downloading the training image..."
     ]
    }
   ],
   "source": [
    "linear_classifier.fit([train_records,\n",
    "                       val_records,\n",
    "                       test_records],\n",
    "                       logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluating the model\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You can use SageMaker analytics to get some performance metrics (of your choosing) on the test set. This process doesn't require you to deploy the model. \n",
    "\n",
    "Linear learner provides metrics that are computed during training. You can use these metrics when tuning the model. The available metrics for the validation set are:\n",
    "\n",
    "- objective_loss - For a binary classification problem, this will be the mean value of the logistic loss for each epoch\n",
    "- binary_classification_accuracy - The accuracy of the final model on the dataset i.e. how many predictions did the model get right\n",
    "- precision - Quantifies the number of positive class predictions that are actually positive\n",
    "- recall - Quantifies the number of positive class predictions\n",
    "- binary_f_beta - The harmonic mean of the precision and recall metrics\n",
    "\n",
    "For this example, you are interested in how many predictions were correct. Using the **binary_classification_accuracy** metric seems appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(linear_classifier._current_job_name, \n",
    "                                         metric_names = ['test:binary_classification_accuracy']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a value of around 0.85. Your value will may be different, but should be around that value. This translates to the model accuractely predicting the correct answer 85% of the time. Depending upon the business case, you may need to tune the model further using a hyperparameter tuning job, or do some more feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deploying the model to an endpoint\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "In this last part of this exercise, you will deploy your model to another instance of your choice. You can use this model in a production environment. Deployed endpoints can be used with other AWS services, such as AWS Lambda and Amazon API Gateway. If you are interested in learning more, see the following walkthrough: [Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/).\n",
    "\n",
    "To deploy the model, run the following cell. You can use different instance types, such as: _ml.t2.medium_, _ml.c4.xlarge_), and others. __This process will take some time to complete (approximately 7-8 minutes).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_classifier_predictor = linear_classifier.deploy(initial_instance_count = 1,\n",
    "                                                       instance_type = 'ml.c5.large'\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Testing the endpoint\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Now that the endpoint is deployed, you will send the test data to it and get predictions from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get test data in batch size of 25 and make predictions.\n",
    "prediction_batches = [linear_classifier_predictor.predict(batch)\n",
    "                      for batch in np.array_split(X_test.astype('float32'), 25)\n",
    "                     ]\n",
    "\n",
    "# Get a list of predictions\n",
    "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleaning up model artifacts\n",
    "([Go to top](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "You can run the following to delete the endpoint after you are done using it. \n",
    "\n",
    "**Tip:** - Remember that when using your own account, you will accrue charges if you don't delete the endpoint and other resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "In this lab, you looked at a very simple NLP problem. Using a labelled dataset, you used a simple tokenizer and encoder to generate the data required to train a linear learner model. You then deployed the model and performed some predictions. If you were doing this for real, you would likely need to obtain the data and label it for training. An alternative might be to use a pretrained algorithm or managed service. You would also likely tune the model further using a hyperparameter tuning job.\n",
    "\n",
    "You have completed this lab, and you can now end the lab by following the lab guide instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "©2023 Amazon Web Services, Inc. or its affiliates. All rights reserved. This work may not be reproduced or redistributed, in whole or in part, without prior written permission from Amazon Web Services, Inc. Commercial copying, lending, or selling is prohibited. All trademarks are the property of their owners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
