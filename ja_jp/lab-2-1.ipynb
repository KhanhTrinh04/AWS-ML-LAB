{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ラボ 2.1: 機械学習の NLP 問題への適用\n",
    "\n",
    "このラボでは、Amazon SageMaker の組み込み機械学習 (ML) モデル (__LinearLearner__) を使って、レビューデータセットの __isPositive__ フィールドを予測します。\n",
    "\n",
    "## ビジネスシナリオの紹介\n",
    "あなたはオンライン小売店で働いており、ネガティブなレビューを投稿した顧客のカスタマーエンゲージメントを向上させたいと思っています。この会社では、ネガティブなレビューを検出し、検出したレビューをカスタマーサービスエージェントに割り当てて対処したいと考えています。\n",
    "\n",
    "あなたは機械学習を使ってネガティブなレビューを検出し、この問題の一部を解決するタスクが割り当てられました。ポジティブまたはネガティブに分類されたレビューを含むデータセットへのアクセス権が与えられています。このデータセットを使って機械学習モデルをトレーニングし、新しいレビューの感情を予測します。\n",
    "\n",
    "## このデータセットについて\n",
    "[AMAZON-REVIEW-DATA-CLASSIFICATION.csv](https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/examples) ファイルには、商品の実際のレビューが含まれており、レビューにはテキストデータと数値データの両方が含まれています。各レビューには、「_positive (1)_」または「_negative (0)_」というラベルが付けられています。\n",
    "\n",
    "データセットには次の特徴が含まれています。\n",
    "* __reviewText:__ レビューのテキスト\n",
    "* __summary:__ レビューの概要\n",
    "* __verified:__ 購入が確認されたかどうか (True または False)\n",
    "* __time:__ レビューの Unix タイムスタンプ\n",
    "* __log_votes:__ 対数調整された投票ログ (1+ 投票)\n",
    "* __isPositive:__ レビューがポジティブかネガティブか (1 または 0)\n",
    "\n",
    "このラボのデータセットは Amazon の許可のもと提供されており、Amazon License and Access (https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000) の規約に準じます。本コースの実施以外の目的でこのデータセットをコピー、変更、販売、エクスポート、使用することは明示的に禁止されています。\n",
    "\n",
    "## ラボのステップ\n",
    "\n",
    "このラボを完了するには、次のステップを実行します。\n",
    "\n",
    "1. [データセットを読み込む](#1.-Reading-the-dataset)\n",
    "2. [探索的データ分析を行う](#2.-Performing-exploratory-data-analysis)\n",
    "3. [テキスト処理: ストップワードを削除し語幹解釈を実行する](#3.-Text-processing:-Removing-stopwords-and-stemming)\n",
    "4. [トレーニング、検証、テストデータを分割する](#4.-Splitting-training,-validation,-and-test-data)\n",
    "5. [パイプラインと ColumnTransformer でデータを処理する](#5.-Processing-data-with-pipelines-and-a-ColumnTransformer)\n",
    "6. [組み込み SageMaker アルゴリズムで分類器をトレーニングする](#6.-Training-a-classifier-with-a-built-in-SageMaker-algorithm)\n",
    "7. [モデルを評価する](#7.-Evaluating-the-model)\n",
    "8. [モデルをエンドポイントにデプロイする](#8.-Deploying-the-model-to-an-endpoint)\n",
    "9. [エンドポイントをテストする](#9.-Testing-the-endpoint)\n",
    "10. [モデルアーティファクトをクリーンアップする](#10.-Cleaning-up-model-artifacts)\n",
    "    \n",
    "## 作業内容を送信する\n",
    "\n",
    "1.ラボコンソールで [**送信**] をクリックして進行状況を記録し、メッセージが表示されたら [**はい**] をクリックします。\n",
    "\n",
    "1.数分経っても結果が表示されない場合は、この手順の上部に戻り、[**Grades**] をクリックします。\n",
    "\n",
    "     **ヒント**: 作業内容は複数回送信できます。作業内容を変更したら、もう一度 [**送信**] をクリックします。最後に送信したものがこのラボの記録として残ります。\n",
    "\n",
    "1.作業内容に関する詳細なフィードバックを参照するには、[**詳細**]、[**View Submission Report**] の順にクリックします。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、pip、sagemaker、scikit-learn をインストールまたはアップグレードします。\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) はオープンソースの機械学習ライブラリです。モデル適合、データの前処理、モデルの選択と評価といった、他種多様なユーティリティツールを提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade botocore\n",
    "!pip install --upgrade awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.データセットを読み込む\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "__pandas__ ライブラリを使ってデータセットを読み込みます。[Pandas] (https://pandas.pydata.org/pandas-docs/stable/index.html) は、データ分析によく使われる python ライブラリです。データ操作、クリーニング、データラングリング、視覚化の機能を備えています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットの最初の 5 行を見てください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノートブックのオプションを変更して、より多くのテキストデータを表示できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要に応じて、特定のエントリを確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[[580]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自分が扱っているデータタイプを確認することをお勧めします。DataFrame で  `dtypes` を使ってタイプを表示できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.探索的データ分析を行う\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "次に、データセットのターゲット分布を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ビジネス上の問題は、ネガティブなレビュー (_0_) を見つけることに焦点を当てています。しかし、線形学習者のモデルチューニングでは、デフォルトで正の値 (_1_) が検索されます。負の値 (_0_) と正の値 (_1_) を切り替えることで、このプロセスをよりスムーズに実行できます。これによりモデルをより簡単にチューニングできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({0:1, 1:0})\n",
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "欠損値の数を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストフィールドには欠損値があります。通常、これらの欠損値をどう処理するかを決定します。データを削除するか、標準テキストを入力できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.テキスト処理: ストップワードを削除し語幹解釈を実行する\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "このタスクでは、ストップワードを削除し、テキストデータのステミングを実行します。データを正規化し、処理する必要のあるさまざまな情報の量を減らします。\n",
    "\n",
    "[nltk](https://www.nltk.org/) は、人間の言語データの処理によく使われるプラットフォームです。分類、トークン化、語幹解釈、タグ付け、構文解析、セマンティック推論のためのテキストを処理するインターフェイスと関数が用意されています。\n",
    "\n",
    "インポートしたら、必要な機能のみをダウンロードできます。この例では次を使います。\n",
    "\n",
    "- **punkt** は文トークナイザです\n",
    "- **stopwords** は使用できるストップワードのリストを提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次のセクションでは、ストップワードを削除してテキストをクリーニングするプロセスを作成します。自然言語ツールキット (NLTK) ライブラリには、一般的なストップワードのリストが用意されています。このリストを使いますが、まずはリストから一部の単語を削除します。テキスト内に保持するストップワードは、感情を判別するのに役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get a list of stopwords from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for your problem. You don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n",
    "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n",
    "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n",
    "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n",
    "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n",
    "\n",
    "# New stopword list\n",
    "stopwords = [word for word in stop if word not in excluding]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "snowball stemmer は単語を語幹にします。例えば、「walking」は「walk」という語幹になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データに対して他にもいくつかの正規化タスクを実行する必要があります。関数と働きは次のとおりです。\n",
    "\n",
    "- 欠損値を空の文字列で置き換える\n",
    "- テキストを小文字に変換する\n",
    "- 先頭または末尾の空白を削除する\n",
    "- 余分なスペースとタブを削除する\n",
    "- HTML マークアップを削除する\n",
    "\n",
    " `for` ループでは、__NOT__ 数値の単語、2 文字より長い単語、ストップワードのリストに含まれない単語はすべて保持され、返されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = ''\n",
    "            \n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # Applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stopwords\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "        \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.トレーニング、検証、テストデータを分割する\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "この手順では、sklearn [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) 関数を使って、データセットをトレーニング (80%)、検証 (10%)、テスト (10%) に分割します。\n",
    "\n",
    "トレーニングデータを使用してモデルをトレーニングし、テストデータでモデルをテストします。モデルのトレーニング完了後、検証セットを使って、モデルの実データでの動作に関するメトリクスを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[['reviewText', 'summary', 'time', 'log_votes']],\n",
    "                                                  df['isPositive'],\n",
    "                                                  test_size=0.20,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=324\n",
    "                                                 )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,\n",
    "                                                y_val,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを分割することで、トレーニングセット、テストセット、検証セットの各テキストの特徴に対して上で定義した  `process_text` 関数を実行できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing the reviewText fields')\n",
    "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n",
    "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n",
    "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n",
    "\n",
    "print('Processing the summary fields')\n",
    "X_train['summary'] = process_text(X_train['summary'].tolist())\n",
    "X_val['summary'] = process_text(X_val['summary'].tolist())\n",
    "X_test['summary'] = process_text(X_test['summary'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.パイプラインと ColumnTransformer でデータを処理する\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "多くの場合、データを使ってモデルをトレーニングする前に、データに対して多くのタスクを実行します。こうしたステップは、モデルのデプロイ後の推論に使われるすべてのデータに対しても実行する必要があります。これらのステップを整理するには、_pipeline_ を定義することをお勧めします。パイプラインはデータに対して実行される一連の処理タスクです。複数のパイプラインを作成して異なるフィールドを処理できます。テキストデータと数値データの両方を操作しているため、次のパイプラインを定義できます。\n",
    "\n",
    "   * 数値特徴のパイプラインの場合、__numerical_processor__ は MinMaxScaler を使います。(決定木を使う場合は特徴をスケールする必要はありませんが、より多くのデータを変換する方法を確認することをお勧めします)。 複数の数値特徴に対して異なるタイプの処理を実行する場合は、2 つのテキスト特徴に表示されるパイプラインのように、複数のパイプラインを構築する必要があります。\n",
    "   * テキスト特徴のパイプラインの場合、__text_processor__ はテキストフィールドに `CountVectorizer()` を使用します。\n",
    "   \n",
    "データセット特徴の選択的前処理は、一連の ColumnTransformer にまとめられ、パイプラインで推定器とともに使用されます。このプロセスにより、モデルを適合させる際や予測を行う際に、raw データに対して変換が自動で確実に実行されます。(例えば、交差検証によって検証データセットのモデルを評価する場合や、将来、テストデータセットで予測を行う場合)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = ['time',\n",
    "                      'log_votes']\n",
    "\n",
    "text_features = ['summary',\n",
    "                 'reviewText']\n",
    "\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'isPositive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) \n",
    "                                ])\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセット内の特徴の数が 4 から 202 にどのように増加したかに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.組み込み SageMaker アルゴリズムで分類器をトレーニングする\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "このステップでは、次のオプションを指定して SageMaker `LinearLearner()` アルゴリズムを呼び出します。\n",
    "* __Permissions-__ `role` は、現在の環境の AWS Identity and Access Management (IAM) ロールに設定します。\n",
    "* __Compute power -__  `train_instance_count` パラメータと `train_instance_type` パラメータを使用します。この例では、 `ml.m4.xlarge` リソースをトレーニングに使用します。インスタンスタイプは必要に応じて変更できます。(例えば、GPU をニューラルネットワークに使用できます)。 \n",
    "* __Model type -__ `predictor_type` は、今はバイナリ分類問題を処理するため、__`binary_classifier`__ に設定します。3 つ以上のクラスが関係している場合、__`multiclass_classifier`__ を使用できます。また、回帰問題には __`regressor`__ を使用できます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "linear_classifier = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                           instance_count=1,\n",
    "                                           instance_type='ml.m4.xlarge',\n",
    "                                           predictor_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推定器のトレーニング、検証、テストの各部分を設定するには、 `binary_estimator` の `record_set()` 関数を使用できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = linear_classifier.record_set(X_train.astype('float32'),\n",
    "                                            y_train.values.astype('float32'),\n",
    "                                            channel='train')\n",
    "val_records = linear_classifier.record_set(X_val.astype('float32'),\n",
    "                                          y_val.values.astype('float32'),\n",
    "                                          channel='validation')\n",
    "test_records = linear_classifier.record_set(X_test.astype('float32'),\n",
    "                                           y_test.values.astype('float32'),\n",
    "                                           channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `fit()` 関数には、確率的勾配降下 (SGD) アルゴリズムの分散されたバージョンが適用されます。そこにデータを送信します。ログは `logs=False` で無効化されました。このパラメータを削除するとプロセスの詳細を表示できます。__このプロセスは ml.m4.xlarge インスタンスで約 3～4 分かかります。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier.fit([train_records,\n",
    "                       val_records,\n",
    "                       test_records],\n",
    "                       logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.モデルを評価する\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "SageMaker の分析を使用して、テストセットの (選択した) パフォーマンスメトリクスを取得できます。このプロセスではモデルをデプロイする必要はありません。\n",
    "\n",
    "線形学習者によってトレーニング中に計算されるメトリクスが作成されます。このメトリクスはモデルをチューニングする際に使用できます。検証セットで使用できるメトリクスは次のとおりです。\n",
    "\n",
    "- objective_loss - バイナリ分類問題の場合、これは各エポックのロジスティック損失の平均値になる\n",
    "- binary_classification_accuracy - データセット上の最終モデルの精度。つまりモデルが適切に取得した予測の数\n",
    "- precision - 真の陽性である陽性クラスの予測の数を定量化する\n",
    "- recall - 陽性クラスの予測の数を定量化する\n",
    "- binary_f_beta - 精度メトリクスと再現率メトリクスの調和平均\n",
    "\n",
    "この例では、正しい予測の数に関心があります。**binary_classification_accuracy** メトリクスを使用するのが適切だと考えられます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(linear_classifier._current_job_name, \n",
    "                                         metric_names = ['test:binary_classification_accuracy']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.85 前後の値が表示されます。値が異なる場合がありますが、この前後の値です。これは、モデルが 85% の時間で正解を正確に予測していることを意味します。ビジネスケースによっては、ハイパーパラメータのチューニングジョブを使用してモデルをさらにチューニングしたり、特徴量エンジニアリングをさらに実行したりする必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.モデルをエンドポイントにデプロイする\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "この演習の最後のセクションでは、選択した別のインスタンスにモデルをデプロイします。このモデルは本番環境で使用できます。デプロイされたエンドポイントは、AWS Lambda や Amazon API Gateway など、他の AWS のサービスで使用できます。詳細については、次のチュートリアルを参照してください。[Call an Amazon SageMaker model endpoint using Amazon API Gateway and AWS Lambda](https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/)\n",
    "\n",
    "モデルをデプロイするには、次のセルを実行します。_ml.t2.medium_、_ml.c4.xlarge_) など、さまざまなインスタンスタイプを使用できます。__このプロセスの完了にはしばらく時間がかかります (約 7～8 分)。__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_classifier_predictor = linear_classifier.deploy(initial_instance_count = 1,\n",
    "                                                       instance_type = 'ml.c5.large'\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.エンドポイントをテストする\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "エンドポイントがデプロイされたので、テストデータをそのエンドポイントに送信し、データから予測を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get test data in batch size of 25 and make predictions.\n",
    "prediction_batches = [linear_classifier_predictor.predict(batch)\n",
    "                      for batch in np.array_split(X_test.astype('float32'), 25)\n",
    "                     ]\n",
    "\n",
    "# Get a list of predictions\n",
    "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.モデルアーティファクトをクリーンアップする\n",
    "([先頭に戻る](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "エンドポイントを使用後、次を実行して削除できます。\n",
    "\n",
    "**ヒント:** - ご自身のアカウントを使用する場合、エンドポイントやその他のリソースを削除しないと料金が発生することに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# お疲れ様でした。\n",
    "\n",
    "このラボでは非常にシンプルな NLP の問題について確認しました。ラベル付きデータセットを使用し、シンプルなトークナイザとエンコーダで線形学習者モデルのトレーニングに必要なデータを生成しました。次に、モデルをデプロイし、いくつかの予測を実行しました。この作業を実際に行っている場合、トレーニング用にデータを取得し、ラベル付けをする必要があります。または、事前トレーニング済みアルゴリズムまたはマネージドサービスを使用することもできます。ハイパーパラメータのチューニングジョブを使用して、モデルをさらにチューニングすることもできます。\n",
    "\n",
    "このラボを完了しました。ラボガイドの手順に従ってラボを終了してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "©2023 Amazon Web Services, Inc. or its affiliates.All rights reserved.このトレーニング内容の全体または一部を複製または再配布することは、Amazon Web Services, Inc. の書面による事前の許可がある場合を除き、禁じられています。商業目的のコピー、貸与、販売を禁止します。すべての商標は各所有者に帰属します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "name": "python385jvsc74a57bd012bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
