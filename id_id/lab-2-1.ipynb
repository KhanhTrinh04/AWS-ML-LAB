{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version: 02.14.2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2.1 - Menerapkan ML untuk Masalah NLP\n",
    "\n",
    "Di lab ini, Anda akan menggunakan model <i>machine learning</i> (ML) bawaan di Amazon SageMaker, __LinearLearner__, untuk memprediksi bidang __isPositive__ dari set data ulasan.\n",
    "\n",
    "\n",
    "## Pengantar skenario bisnis\n",
    "Anda bekerja untuk toko ritel online yang ingin meningkatkan keterlibatan pelanggan bagi pelanggan yang telah memposting ulasan negatif. Perusahaan ingin mendeteksi ulasan negatif dan menetapkan ulasan ini ke agen layanan pelanggan untuk diatasi.\n",
    "\n",
    "Anda ditugaskan untuk memecahkan bagian dari masalah ini dengan menggunakan ML untuk mendeteksi ulasan negatif. Anda diberi akses ke set data yang berisi ulasan, yang telah diklasifikasikan sebagai positif atau negatif. Anda akan menggunakan set data ini untuk melatih model ML untuk memprediksi sentimen ulasan baru.\n",
    "\n",
    "## Tentang set data ini\n",
    "File [AMAZON-REVIEW-DATA-CLASSIFICATION.csv] (https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp/tree/master/data/examples) berisi ulasan aktual produk, dan ulasan ini mencakup data teks dan data numerik. Setiap ulasan diberi label sebagai _positive (1)_ atau _negative (0)_.\n",
    "\n",
    "\n",
    "Set data berisi fitur berikut:\n",
    "* __reviewText:__ Teks ulasan\n",
    "* __summary:__ Ringkasan ulasan\n",
    "* __verified:__ Apakah pembelian telah diverifikasi (Benar atau Salah)\n",
    "* __time:__ <i>Timestamp</i> UNIX untuk ulasan\n",
    "* __log_votes:__ Log <i>vote</i> yang disesuaikan algoritme(1+<i>vote</i>)\n",
    "* __isPositive:__ Apakah ulasan positif atau negatif (1 atau 0)\n",
    "\n",
    "Set data untuk lab ini disediakan untuk Anda dengan izin dari Amazon dan tunduk pada persyaratan Lisensi dan Akses Amazon (tersedia di https://www.amazon.com/gp/help/customer/display.html?nodeId=201909000). Anda secara tegas dilarang menyalin, memodifikasi, menjual, mengekspor, atau menggunakan set data ini dengan cara apa pun selain untuk tujuan menyelesaikan kursus ini.\n",
    "\n",
    "## Langkah-langkah lab\n",
    "\n",
    "Untuk menyelesaikan lab, Anda akan mengikuti langkah-langkah berikut:\n",
    "\n",
    "1. [Membaca set data](#1.-Reading-the-dataset)\n",
    "2. [Melakukan analisis data eksplorasi](#2.-Performing-exploratory-data-analysi)\n",
    "3. [Pemrosesan teks: Menghapus <i>stopwords </i>dan <i>stemming</i>](#3.-Pemrosesan teks:-Removing-stopwords-and-stemming)\n",
    "4. [Memisahkan data pelatihan, validasi, dan tes](#4.-Splitting-training,-validation,-and-test-data)\n",
    "5. [Memproses data dengan alur dan ColumnTransformer](#5.-Processing-data-with-pipelines-and-a-ColumnTransformer)\n",
    "6. [Melatih pengklasifikasi dengan algoritma SageMaker bawaan](#6.-Training-a-classifier-with-a-built-in-SageMaker-algorithm)\n",
    "7. [Mengevaluasi model](#7.-Evaluating-the-model)\n",
    "8. [Men-<i>deploy </i>model ke titik akhir](#8.-Deploying-the-model-to-an-endpoint)\n",
    "9. [Menguji titik akhir](#9.-Testing-the-endpoint)\n",
    "10. [Membersihkan artifacts model](#10.-Cleaning-up-model-artifacts)\n",
    "    \n",
    "## Mengirimkan pekerjaan Anda\n",
    "\n",
    "1. Di konsol lab, pilih **<i>Submit</i>** (Kirim) untuk merekam kemajuan Anda dan saat diminta, pilih **<i>Yes</i>** (Ya).\n",
    "\n",
    "1. Jika hasilnya tidak muncul setelah beberapa menit, kembali ke bagian atas petunjuk ini dan pilih <i>**Grades**</i> (Nilai).\n",
    "\n",
    "     **Tip**: Anda dapat mengirimkan pekerjaan Anda beberapa kali. Setelah Anda mengubah pekerjaan Anda, pilih **<i>Submit</i>** (Kirim) lagi. Kiriman terakhir Anda akan direkam untuk lab ini.\n",
    "\n",
    "1. Untuk menemukan detail umpan balik tentang pekerjaan Anda, pilih <i>**Details** </i>(Detail) yang diikuti dengan **<i>View Submission Report</i>** (Lihat Laporan Pengiriman).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mulailah dengan menginstal/memutakhirkan pip, sagemaker, dan scikit-learn.\n",
    "\n",
    "[scikit-learn](https://scikit-learn.org/stable/) adalah pustaka <i>machine learning</i> sumber terbuka. Pustaka menyediakan berbagai alat untuk penyesuaian model, pemrosesan data di awal, pemilihan dan evaluasi model, serta berbagai utilitas lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install --upgrade sagemaker\n",
    "!pip install --upgrade botocore\n",
    "!pip install --upgrade awscli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Membaca set data\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Anda akan menggunakan pustaka __pandas__ untuk membaca set data. [pandas] (https://pandas.pydata.org/pandas-docs/stable/index.html) adalah pustaka python populer yang digunakan untuk analisis data. Pustaka menyediakan fitur manipulasi, pembersihan, dan perselisihan serta visualisasi data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/AMAZON-REVIEW-DATA-CLASSIFICATION.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lihatlah lima baris pertama dalam set data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda dapat mengubah opsi di <i>notebook</i> untuk menampilkan lebih banyak data teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda dapat melihat entri tertentu jika diperlukan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[[580]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ada baiknya mengetahui tipa data apa yang Anda hadapi. Anda dapat menggunakan `dtypes` pada dataframe untuk menampilkan tipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Melakukan analisis data eksplorasi\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Anda sekarang akan melihat distribusi target untuk set data Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masalah bisnis yang terkait dengan menemukan ulasan negatif (_0_). Namun, penyetelan model untuk pelajar linier secara <i>default </i>diatur untuk menemukan nilai positif (_1_).\n",
    " Anda dapat membuat proses ini berjalan lebih lancar dengan mengalihkan nilai negatif (_0_) dan nilai-nilai positif (_1_). Dengan demikian, Anda bisa menyetel model dengan lebih mudah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({0:1, 1:0})\n",
    "df['isPositive'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periksa jumlah nilai yang hilang:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidang teks memiliki nilai yang hilang. Biasanya, Anda akan memutuskan apa yang harus dilakukan dengan nilai-nilai yang hilang ini. Anda dapat menghapus data atau mengisinya dengan beberapa teks standar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pemrosesan teks: Menghapus <i>stopword</i> dan <i>stemming</i>\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Dalam tugas ini, Anda akan menghapus beberapa <i>stopword</i>, dan melakukan <i>stemming </i>pada data teks.\n",
    " Anda menormalkan data untuk mengurangi jumlah informasi berbeda yang harus Anda hadapi.\n",
    "\n",
    "[nltk](https://www.nltk.org/) adalah platform yang populer untuk bekerja dengan data bahasa manusia. Platform menyediakan berbagai antarmuka dan fungsi untuk memproses teks untuk klasifikasi, pemberian token, <i>stemming</i>, pemberian tanda, penguraian, dan penalaran semantik. \n",
    "\n",
    "Setelah diimpor, Anda hanya dapat mengunduh fungsionalitas yang Anda butuhkan. Dalam contoh ini, Anda akan menggunakan:\n",
    "\n",
    "- **<i>punkt</i>** adalah pembuat token kalimat\n",
    "- **<i>stopword</i>** menyediakan daftar stopword yang dapat Anda gunakan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library and functions\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda akan membuat proses untuk menghapus <i>stopword</i> dan membersihkan teks di bagian berikut. Pustaka <i>Natural Language Toolkit</i> (NLTK) menyediakan daftar <i>stopword</i> umum. Anda akan menggunakan daftar, tetapi Anda akan terlebih dulu menghapus beberapa kata dari daftar tersebut. <i>Stopword</i> yang Anda simpan dalam teks berguna untuk menentukan sentimen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Get a list of stopwords from the NLTK library\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These words are important for your problem. You don't want to remove them.\n",
    "excluding = ['against', 'not', 'don', 'don\\'t','ain', 'are', 'aren\\'t', 'could', 'couldn\\'t',\n",
    "             'did', 'didn\\'t', 'does', 'doesn\\'t', 'had', 'hadn\\'t', 'has', 'hasn\\'t', \n",
    "             'have', 'haven\\'t', 'is', 'isn\\'t', 'might', 'mightn\\'t', 'must', 'mustn\\'t',\n",
    "             'need', 'needn\\'t','should', 'shouldn\\'t', 'was', 'wasn\\'t', 'were', \n",
    "             'weren\\'t', 'won\\'t', 'would', 'wouldn\\'t']\n",
    "\n",
    "# New stopword list\n",
    "stopwords = [word for word in stop if word not in excluding]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Stemmer</i> snowball akan memangkas kata-kata. Misalnya, 'berjalan' akan dipangkas ke 'jalan'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda harus melakukan beberapa tugas normalisasi lainnya pada data. Fungsi berikut akan:\n",
    "\n",
    "- Mengganti nilai yang hilang dengan string kosong\n",
    "- Mengonversi teks ke huruf kecil\n",
    "- Menghapus spasi di depan atau di belakang kalimat\n",
    "- Menghapus ruang dan tab ekstra\n",
    "- Menghapus <i>markup</i> HTML\n",
    "\n",
    "Dalam lingkaran  `for`, kata-kata __NOT__ numeric, lebih panjang dari 2 karakter, dan bukan bagian dari daftar kata henti akan disimpan dan dikembalikan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(texts): \n",
    "    final_text_list=[]\n",
    "    for sent in texts:\n",
    "        \n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = ''\n",
    "            \n",
    "        filtered_sentence=[]\n",
    "        \n",
    "        sent = sent.lower() # Lowercase \n",
    "        sent = sent.strip() # Remove leading/trailing whitespace\n",
    "        sent = re.sub('\\s+', ' ', sent) # Remove extra space and tabs\n",
    "        sent = re.compile('<.*?>').sub('', sent) # Remove HTML tags/markups:\n",
    "        \n",
    "        for w in word_tokenize(sent):\n",
    "            # Applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stopwords\n",
    "            if(not w.isnumeric()) and (len(w)>2) and (w not in stopwords):  \n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence) # Final string of cleaned words\n",
    " \n",
    "        final_text_list.append(final_string)\n",
    "        \n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memisahkan data pelatihan, validasi, dan tes\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Pada langkah ini, Anda akan membagi set data ke pelatihan (80 persen), validasi (10 persen), dan tes (10 persen) dengan menggunakan fungsi sklearn [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "Data pelatihan akan digunakan untuk melatih model yang kemudian diuji dengan data tes. Set validasi digunakan setelah model telah dilatih untuk memberikan metrik tentang bagaimana model dapat menunjukkan performa pada data sungguhan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[['reviewText', 'summary', 'time', 'log_votes']],\n",
    "                                                  df['isPositive'],\n",
    "                                                  test_size=0.20,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=324\n",
    "                                                 )\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val,\n",
    "                                                y_val,\n",
    "                                                test_size=0.5,\n",
    "                                                shuffle=True,\n",
    "                                                random_state=324)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dengan memisahkan set data, kini Anda dapat menjalankan fungsi  `process_text` fungsi yang didefinisikan di atas pada masing-masing fitur teks dalam set pelatihan, tes, dan validasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processing the reviewText fields')\n",
    "X_train['reviewText'] = process_text(X_train['reviewText'].tolist())\n",
    "X_val['reviewText'] = process_text(X_val['reviewText'].tolist())\n",
    "X_test['reviewText'] = process_text(X_test['reviewText'].tolist())\n",
    "\n",
    "print('Processing the summary fields')\n",
    "X_train['summary'] = process_text(X_train['summary'].tolist())\n",
    "X_val['summary'] = process_text(X_val['summary'].tolist())\n",
    "X_test['summary'] = process_text(X_test['summary'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memproses data dengan alur dan ColumnTransformer\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Anda akan sering melakukan banyak tugas pada data sebelum Anda menggunakannya untuk melatih model. Langkah-langkah ini juga harus dilakukan pada setiap data yang digunakan untuk inferensi setelah model di-<i>deploy</i>. Cara yang baik untuk mengatur langkah-langkah ini adalah dengan menentukan _pipeline_. Alur adalah kumpulan tugas pemrosesan yang akan dilakukan pada data. Alur berbeda dapat dibuat untuk memproses bidang yang berbeda. Karena Anda bekerja dengan data teks dan numerik, Anda dapat menentukan alur berikut:\n",
    "\n",
    "   * Untuk alur fitur numerik, __numerical_processor__ menggunakan MinMaxScaler. (Anda tidak perlu menskalakan fitur saat menggunakan silsilah keputusan, tetapi sebaiknya lihat cara menggunakan lebih banyak perubahan data.) Jika Anda ingin melakukan berbagai tipe pemrosesan pada fitur numerik yang berbeda, Anda sebaiknya membangun alur yang berbeda, seperti yang ditampilkan untuk dua fitur teks.\n",
    "   * Untuk alur fitur teks __text_processor__ menggunakan  `CountVectorizer()` untuk bidang teks.\n",
    "   \n",
    "Persiapan selektif dari fitur set data kemudian disatukan ke dalam ColumnTransformer kolektif, yang akan digunakan dengan dalam alur bersama dengan estimator. Proses ini memastikan bahwa transformasi dilakukan secara otomatis pada data mentah saat Anda menyesuaikan model atau membuat prediksi. (Misalnya, saat Anda mengevaluasi model pada set data validasi melalui validasi silang, atau saat Anda membuat prediksi pada set data tes di masa mendatang.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "numerical_features = ['time',\n",
    "                      'log_votes']\n",
    "\n",
    "text_features = ['summary',\n",
    "                 'reviewText']\n",
    "\n",
    "model_features = numerical_features + text_features\n",
    "model_target = 'isPositive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) \n",
    "                                ])\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "### DATA PREPROCESSING ###\n",
    "##########################\n",
    "\n",
    "print('Datasets shapes before processing: ', X_train.shape, X_val.shape, X_test.shape)\n",
    "\n",
    "X_train = data_preprocessor.fit_transform(X_train).toarray()\n",
    "X_val = data_preprocessor.transform(X_val).toarray()\n",
    "X_test = data_preprocessor.transform(X_test).toarray()\n",
    "\n",
    "print('Datasets shapes after processing: ', X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhatikan bagaimana jumlah fitur dalam set data berubah dari 4 menjadi 202."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Melatih pengklasifikasi dengan algoritma SageMaker bawaan\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Pada langkah ini, Anda akan memanggil algoritma  `LinearLearner()` Sagemaker dengan opsi berikut:\n",
    "* __Permissions -__ `role` diatur ke <i>AWS Identity and Access Management</i> (IAM) role dari lingkungan saat ini.\n",
    "* __Compute power -__ Anda akan menggunakan parameter `train_instance_count` dan parameter `train_instance_type`. Contoh ini menggunakan sumber daya `ml.m4.xlarge` untuk pelatihan. Anda dapat mengubah tipe instans tergantung kebutuhan Anda. (Misalnya, Anda dapat menggunakan GPU untuk jaringan neural.) \n",
    "* __Model type -__ `predictor_type` diatur ke __`binary_classifier`__ karena Anda bekerja dengan masalah klasifikasi biner. Anda dapat menggunakan __`multiclass_classifier`__ jika tiga atau lebih kelas terlibat, atau Anda dapat menggunakan __`regressor`__ untuk masalah regresi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# Call the LinearLearner estimator object\n",
    "linear_classifier = sagemaker.LinearLearner(role=sagemaker.get_execution_role(),\n",
    "                                           instance_count=1,\n",
    "                                           instance_type='ml.m4.xlarge',\n",
    "                                           predictor_type='binary_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk mengatur bagian pelatihan, validasi, dan tes estimator, Anda dapat menggunakan fungsi `record_set()`  `binary_estimator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_records = linear_classifier.record_set(X_train.astype('float32'),\n",
    "                                            y_train.values.astype('float32'),\n",
    "                                            channel='train')\n",
    "val_records = linear_classifier.record_set(X_val.astype('float32'),\n",
    "                                          y_val.values.astype('float32'),\n",
    "                                          channel='validation')\n",
    "test_records = linear_classifier.record_set(X_test.astype('float32'),\n",
    "                                           y_test.values.astype('float32'),\n",
    "                                           channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi `fit()` menerapkan versi algoritma <i>Stochastic Gradient Descent</i> (SGD), dan Anda mengirim data ke algoritma ini. Log dinonaktifkan dengan `logs=False`. Anda dapat menghapus parameter tersebut untuk melihat detail selengkapnya tentang proses. __Proses ini memerlukan waktu sekitar 3-4 menit pada instans ml.m4.xlarge. __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier.fit([train_records,\n",
    "                       val_records,\n",
    "                       test_records],\n",
    "                       logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Mengevaluasi model\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Anda dapat menggunakan analisis SageMaker untuk mendapatkan beberapa metrik performa (yang Anda pilih) pada set tes. Proses ini tidak mengharuskan Anda untuk men-deploy model. \n",
    "\n",
    "Peserta linier menyediakan metrik yang dikomputasi selama pelatihan. Anda dapat menggunakan metrik ini saat menyetel model. Metrik yang tersedia untuk set validasi adalah:\n",
    "\n",
    "- objective_loss - Untuk masalah klasifikasi biner, ini akan menjadi nilai rata-rata untuk kehilangan logistik untuk setiap <i>epoch</i>\n",
    "- binary_classification_accuracy - Keakuratan model akhir pada set data, yaitu berapa banyak prediksi yang dibuat oleh model dengan benar\n",
    "\n",
    "- precision - Menghitung jumlah prediksi kelas positif yang sebenarnya positif\n",
    "- recall - Menghitung jumlah prediksi kelas positif\n",
    "- binary_f_beta - Nilai rata-rata metrik presisi dan pemanggilan ulang\n",
    "\n",
    "Untuk contoh ini, Anda tertarik dengan jumlah prediksi yang benar. Menggunakan metrik **binary_classification_accuracy** tampaknya menjadi pilihan yang tepat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.analytics.TrainingJobAnalytics(linear_classifier._current_job_name, \n",
    "                                         metric_names = ['test:binary_classification_accuracy']\n",
    "                                        ).dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anda seharusnya melihat nilai sekitar 0,85. Nilai Anda mungkin berbeda, tetapi seharusnya di sekitar nilai tersebut. Nilai ini berarti model secara akurat memprediksi jawaban yang benar sebanyak 85%. Tergantung kasus bisnis, Anda mungkin perlu menyetel model lebih lanjut menggunakan pekerjaan penyetelan<i> hyperparameter</i>, atau melakukan beberapa rekayasa fitur lainnya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Men-<i>deploy</i> model ke titik akhir\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Dalam bagian terakhir dari latihan ini, Anda akan men-<i>deploy</i> model Anda untuk instans lain pilihan Anda. Anda dapat menggunakan model ini di lingkungan produksi. Titik akhir yang di-<i>deploy</i> dapat digunakan dengan layanan AWS lainnya, seperti AWS Lambda dan Amazon API Gateway. Jika Anda tertarik untuk mempelajari selengkapnya, lihat panduan berikut: [Panggil titik akhir model Amazon SageMaker menggunakan Amazon API Gateway dan AWS Lambda] (https://aws.amazon.com/blogs/machine-learning/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda/).\n",
    "\n",
    "Untuk men-<i>deploy</i> model, jalankan sel berikut. Anda dapat menggunakan tipe instans berbeda, seperti: _ml.t2.medium_, _ml.c4.xlarge_), dan lainnya. __Proses ini akan memerlukan waktu beberapa saat untuk diselesaikan (sekitar 7-8 menit).__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_classifier_predictor = linear_classifier.deploy(initial_instance_count = 1,\n",
    "                                                       instance_type = 'ml.c5.large'\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Menguji titik akhir\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Sekarang setelah titik akhir di-<i>deploy</i>, Anda akan mengirim data tes ke titik akhir dan mendapatkan prediksi dari data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get test data in batch size of 25 and make predictions.\n",
    "prediction_batches = [linear_classifier_predictor.predict(batch)\n",
    "                      for batch in np.array_split(X_test.astype('float32'), 25)\n",
    "                     ]\n",
    "\n",
    "# Get a list of predictions\n",
    "print([pred.label['score'].float32_tensor.values[0] for pred in prediction_batches[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Membersihkan model artefak\n",
    "([Pergi ke bagian atas](#Lab-2.1:-Applying-ML-to-an-NLP-Problem))\n",
    "\n",
    "Anda dapat menjalankan berikut untuk menghapus titik akhir setelah Anda selesai menggunakannya. \n",
    "\n",
    "**Kiat: ** - Ingat bahwa saat menggunakan akun Anda sendiri, Anda akan dikenai biaya tambahan jika tidak menghapus titik akhir dan sumber daya lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_classifier_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selamat!\n",
    "\n",
    "Di lab ini, Anda melihat masalah NLP yang sangat sederhana. Dengan menggunakan set data berlabel, Anda menggunakan pembuat token dan <i>encoder </i>sederhana untuk menghasilkan data yang dibutuhkan untuk melatih model pelajar linier.\n",
    " Anda kemudian men-<i>deploy</i> model dan melakukan beberapa prediksi. Jika Anda benar-benar melakukan, Anda mungkin akan perlu mendapatkan data dan melabelinya untuk pelatihan. Alternatif mungkin menggunakan algoritma yang telah dilatih sebelumnya atau layanan terkelola. Anda juga mungkin akan menyetel model lebih lanjut menggunakan pekerjaan penyetelan <i>hyperparameter</i>.\n",
    "\n",
    "Anda telah menyelesaikan lab ini, dan sekarang Anda dapat mengakhirinya dengan mengikuti petunjuk panduan lab tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Â©2021, Amazon Web Services, Inc. atau afiliasinya. Hak cipta dilindungi undang-undang. Karya ini tidak boleh direproduksi atau didistribusikan ulang, seluruhnya atau sebagian, tanpa izin tertulis sebelumnya dari Amazon Web Services, Inc. Dilarang menyalin, meminjamkan, atau menjual secara komersial. Semua merek dagang adalah kepemilikan dari pemiliknya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "name": "python385jvsc74a57bd012bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "12bdb53ebf8de4a8c3e84b62f6391946884c7c7585d9344b706f290a85145ccc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
